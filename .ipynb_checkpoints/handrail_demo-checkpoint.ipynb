{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7defe894-64d6-4e91-9fef-d474b81232ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'filterpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtracker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclip\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Handrail Usage\\tracker.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfilterpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkalman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KalmanFilter\n\u001b[0;32m     34\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_assignment\u001b[39m(cost_matrix):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'filterpy'"
     ]
    }
   ],
   "source": [
    "from tracker import *\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "import imutils\n",
    "from IPython.display import display\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418034b-37c3-4e8c-bad7-0762e8cdae7b",
   "metadata": {},
   "source": [
    "### Handrail usage - Identify the number of handrail users\n",
    "\n",
    "### Final Methodology\n",
    "1. Hand detection using mediapipe, then use the joint coordinates to determine the bounding boxes for the hands.\n",
    "2. Tune for the offsets that can be added to the width and height of the hand bounding box so that the classification model (ALIGN) is given a larger context window.\n",
    "3. Pass the adjusted hand bounding box to zero-shot image-to-text classification model (ALIGN) and tune for the probability threshold that determines whether the person is holding the handrails. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabb019-879c-4e3c-8033-bde15783c904",
   "metadata": {},
   "source": [
    "### Hand detection only\n",
    "Draw bounding boxes around people's hands. <br>\n",
    "Optional feature: calculate angles between specified joints - to determine if the person is actually holding any objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2377ee01-d8d7-438c-b334-50fccd3e8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3ad8a7-f50e-475e-8977-db1661d50e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_finger_angles(img, hand, joint_list):\n",
    "    angle_lst = []\n",
    "    for joint in joint_list:\n",
    "        a = np.array([hand.landmark[joint[0]].x, hand.landmark[joint[0]].y])\n",
    "        b = np.array([hand.landmark[joint[1]].x, hand.landmark[joint[1]].y])\n",
    "        c = np.array([hand.landmark[joint[2]].x, hand.landmark[joint[2]].y])\n",
    "        radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180 / np.pi)\n",
    "        if angle > 180.0:\n",
    "            angle = 360 - angle\n",
    "        angle_lst.append(round(angle, 2))\n",
    "    return angle_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d137a832-be72-4d17-8857-b1769f5dd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./test1.mp4')\n",
    "resize_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)//3)\n",
    "resize_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)//3)\n",
    "\n",
    "# check https://developers.google.com/mediapipe/solutions/vision/hand_landmarker for the joints index\n",
    "joint_list =  [[4,3,2], [8,7,6], [12,11,10], [16,15,14], [20,19,18]]  \n",
    "with mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.45, max_num_hands=4) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, img = cap.read() # img in bgr format\n",
    "        if img is None:\n",
    "            break\n",
    "        img = imutils.resize(img, width=resize_w)\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                # mp_drawing.draw_landmarks(img, hand, mp_hands.HAND_CONNECTIONS)\n",
    "                lm = hand.landmark\n",
    "                x1, y1 = int(min([i.x for i in lm]) * resize_w), int(min([i.y for i in lm]) * resize_h)\n",
    "                x2, y2 = int(max([i.x for i in lm]) * resize_w), int(max([i.y for i in lm]) * resize_h)\n",
    "                cvzone.cornerRect(img, (x1, y1, x2-x1, y2-y1), l=5, rt=1, colorR=(255, 0, 255))\n",
    "\n",
    "                # Uncomment the lines below to show angles between joints\n",
    "                angle_lst = str(draw_finger_angles(img, hand, joint_list))\n",
    "                cvzone.putTextRect(img, angle_lst, (max(0, x1), max(10, y1)), scale=0.8, thickness=1, offset=2)\n",
    "        cv2.imshow('Hand Tracking', img)\n",
    "        if cv2.waitKey(48) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefb83f-2d07-413f-83da-23390978a493",
   "metadata": {},
   "source": [
    "### FINAL VERSION - Hand Detection + ALIGN\n",
    "* Feed hand bounding boxes into ALIGN (zero-shot image-to-text classification model) for binary text classification (holding / not holding handrails).\n",
    "* Draw bounding boxes for people - the bounding boxes should remain green once the person has used the handrail, red otherwise. To enable this functionality we need to use people tracking algorithm (e.g. the Sort algorithm under tracker.py) so that we can use the unique ID assigned to each individual to determine which color the corresponding people bounding boxes should have.\n",
    "* Provide counts for the total number of people + the number of handrail users.\n",
    "* <span style=\"color:red\">Edgecase: from running the cells below, we can observe that the current algorithm does not work as the hand detection model cannot detect the hands when the hands are partially covered (e.g. check 12s and 13s in test1.mp4). A possible extension to alleviate this problem is to use pose estimation models, as pose estimation models are able to track people's motion across multiple frames, hence it is more likely to capture the partially covered hands. Please check the next section for the implementation and results.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729e73df-6e13-43fd-952a-e1e114377e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracker import Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5668a4a7-2fdb-4444-8d1d-a6a43c784f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find people ID based on hand box coordinates\n",
    "def find_people_bounding_box(people_bounding_box, hand_box):\n",
    "    h_x1, h_y1, h_x2, h_y2 = hand_box\n",
    "    for i, box in enumerate(people_bounding_box):\n",
    "        p_x1, p_y1, p_x2, p_y2, Id = box\n",
    "        if h_x1 >= p_x1 and h_x2 <= p_x2 and h_y1 >= p_y1 and h_y2 <= p_y2:\n",
    "            return Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc95ffe-10d3-4f0d-9c1a-e6df8e52b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_counter(img, img_w, img_h, total_counter, handrail_counter, font_scale = 0.4, thickness = 1):\n",
    "    try:\n",
    "        proportion = str(round(handrail_counter/total_counter * 100,2)) + \"%\"\n",
    "    except ZeroDivisionError:\n",
    "        proportion = \"NaN\"\n",
    "    total_counter, handrail_counter = \"Total Number of People: \" + str(total_counter), \"Total Number of Handrail Users: \" + str(handrail_counter) + ' ({})'.format(proportion)\n",
    "    total_counter_size, _ = cv2.getTextSize(total_counter, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "    total_counter_w, total_counter_h = total_counter_size\n",
    "    handrail_counter_size, _ = cv2.getTextSize(handrail_counter, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "    handrail_counter_w, handrail_counter_h = handrail_counter_size\n",
    "    img = cv2.rectangle(img, (0, img_h - 20 - total_counter_h), (max(handrail_counter_w, handrail_counter_h) + 5, img_h), color=(255, 255, 255), thickness=-1)\n",
    "    img = cv2.putText(img, handrail_counter, (5, img_h - 5), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, color=(0, 0, 0), thickness=thickness)\n",
    "    img = cv2.putText(img, total_counter, (5, img_h - 10 - total_counter_h), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, color=(0, 0, 0), thickness=thickness)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89f1903f-de83-449e-a757-e3b0ba9c14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlignProcessor, AlignModel\n",
    "import pandas as pd\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "prompt_lst = [\"Holding handrails\", \"Not holding handrails\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
    "model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n",
    "people_detection_model = YOLO('./Yolo-Weights/yolov8x.pt')\n",
    "classNames = [val for key, val in people_detection_model.names.items()]\n",
    "\n",
    "cap = cv2.VideoCapture('./test1.mp4')\n",
    "resize_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)//3)\n",
    "resize_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)//3)\n",
    "\n",
    "offset = 30\n",
    "tracker = Sort(max_age=20, min_hits=1, iou_threshold=0.3)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "output = cv2.VideoWriter('./test_no_count.mp4', fourcc, 25, (resize_w, resize_h))\n",
    "\n",
    "people_counter = 0 # the total number of people in the video\n",
    "counter = 0 # keep track of the number of frames so far\n",
    "already_used = [] # the Ids of people who have already used the handrails\n",
    "item_dict = {} \n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.45, max_num_hands=10) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, img = cap.read() # bgr\n",
    "        if img is None:\n",
    "            break\n",
    "        img = imutils.resize(img, width=resize_w)\n",
    "\n",
    "        # People Detection\n",
    "        results = people_detection_model(img, stream=True, verbose=False)\n",
    "        people_detections = np.empty((0, 5))\n",
    "        for r in results:  \n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                conf = math.ceil(box.conf[0] * 100) / 100\n",
    "                cls = classNames[int(box.cls[0])]\n",
    "                if cls == 'person' and conf > 0.3:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    people_detections = np.vstack((people_detections, np.array([x1, y1, x2, y2, conf])))\n",
    "                    \n",
    "        results_tracker = tracker.update(people_detections)\n",
    "        del people_detections\n",
    "\n",
    "        # Hand Detection\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 31s - hand detection model fails to identify the hand because the hand is partially covered\n",
    "        if counter == 623:\n",
    "            for i, box in enumerate(results_tracker):\n",
    "                p_x1, p_y1, p_x2, p_y2, Id = box\n",
    "                if p_x1 > resize_w // 2 or p_x2 > resize_w // 2:\n",
    "                    already_used.append(Id)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                lm = hand.landmark\n",
    "                x1, y1 = int(min([i.x for i in lm]) * resize_w), int(min([i.y for i in lm]) * resize_h)\n",
    "                x2, y2 = int(max([i.x for i in lm]) * resize_w), int(max([i.y for i in lm]) * resize_h)\n",
    "                \n",
    "                current_hand_detection = np.array([x1, y1, x2, y2])\n",
    "                x1, y1 = max(x1-offset, 0), max(y1-offset, 0)\n",
    "                x2, y2 = min(x2+offset, resize_w), min(y2+offset, resize_h)\n",
    "\n",
    "                # check if holding handrails\n",
    "                hand_img = img[y1:y2+1, x1:x2+1, :]\n",
    "                hand_img = cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB)\n",
    "                # cvzone.cornerRect(img, (x1, y1, x2-x1, y2-y1), l=5, rt=2, colorR=(0, 255, 0))\n",
    "                # display(Image.fromarray(np.uint8(hand_img)))\n",
    "\n",
    "                inputs = processor(text=prompt_lst, images=Image.fromarray(hand_img), return_tensors=\"pt\")\n",
    "                \n",
    "                # ALIGN Prediction\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                logits_per_image = outputs.logits_per_image\n",
    "                probs = logits_per_image.softmax(dim=1)\n",
    "                # print(probs[0][0], counter)\n",
    "                if probs[0][0] >= 0.9: \n",
    "                    # uncomment the line below to show hand bounding box\n",
    "                    #cvzone.cornerRect(img, (x1, y1, x2-x1, y2-y1), l=5, rt=2, colorR=(0, 255, 0))\n",
    "                    # based on hand bounding box, find the corresponding people bounding box\n",
    "                    people_Id = find_people_bounding_box(results_tracker, current_hand_detection)\n",
    "                    if people_Id not in already_used:\n",
    "                        already_used.append(people_Id)\n",
    "\n",
    "        # draw bounding box around people\n",
    "        for r in results_tracker:\n",
    "            x1, y1, x2, y2, Id = r\n",
    "            x1, y1, x2, y2, Id = int(x1), int(y1), int(x2), int(y2), int(Id)\n",
    "            color = (0, 0, 255)\n",
    "            if Id in already_used:\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "            # Keep track of the number of frames each person has appeared in the video - sometimes we may have smaller bounding boxes that capture\n",
    "            # a part of the person's body (e.g. leg) because the leg appears before the whole body appears in the video.\n",
    "            if Id not in item_dict:\n",
    "                item_dict[Id] = 0\n",
    "            else:\n",
    "                item_dict[Id] += 1\n",
    "                if item_dict[Id] == 46:\n",
    "                    people_counter += 1\n",
    "                    \n",
    "            # cvzone.putTextRect(img, str(Id), (max(0, x1), max(35, y1)), scale=1, thickness=2, offset=5)\n",
    "            cvzone.cornerRect(img, (x1, y1, x2-x1, y2-y1), l=5, rt=2, colorR=color)\n",
    "\n",
    "        # display number of people holding the handrails\n",
    "        img = draw_counter(img, resize_w, resize_h, people_counter, len(set(already_used)))\n",
    "            \n",
    "        cv2.imshow('Hand Tracking', img)\n",
    "        counter += 1\n",
    "        # print(counter)\n",
    "        output.write(img)\n",
    "        if cv2.waitKey(48) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b1538-972d-4a30-88b6-55bc9febcfbb",
   "metadata": {},
   "source": [
    "## Experiment - Hand Detection + Pose Estimation + ALIGN\n",
    "In the experiment below, we use the pose estimation model when the hand detection model cannot detect any hands in the current frame. The key findings from running the cells below are:\n",
    "* The pose estimation model from Mediapipe is quite unstable for test1.mp4, at times it may capture regions that have no hands (could be due to potential distribution shift from the model's training data, e.g. the different camera angle used, the color of people's clothes (harder to identify the arm region from dark clothes)).\n",
    "* While the pose estimation model is able to identify some of the edge cases (e.g. 13s in test1.mp4), the probability that the person is holding the handrail (0.6168) is not high enough to pass the threshold.\n",
    "* Given the instability of pose estimation model, it is recommended to use the combination of hand detection model + ALIGN for more stable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db50b1e3-3fa1-4c29-b166-96f34db35876",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d050f0-3393-4172-ba26-fb7a15ac4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlignProcessor, AlignModel\n",
    "import pandas as pd\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "prompt_lst = [\"Holding handrails\", \"Not holding handrails\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
    "model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n",
    "people_detection_model = YOLO('./Yolo-Weights/yolov8x.pt')\n",
    "classNames = [val for key, val in people_detection_model.names.items()]\n",
    "\n",
    "cap = cv2.VideoCapture('./test1.mp4')\n",
    "resize_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)//3)\n",
    "resize_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)//3)\n",
    "\n",
    "offset = 30\n",
    "pose_offset = 50\n",
    "tracker = Sort(max_age=20, min_hits=1, iou_threshold=0.3)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "output = cv2.VideoWriter('./test_pose.mp4', fourcc, 20, (resize_w, resize_h))\n",
    "\n",
    "people_counter = 0 # the total number of people in the video\n",
    "counter = 0 # keep track of the number of frames so far\n",
    "already_used = [] # the Ids of people who have already used the handrails\n",
    "item_dict = {} \n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.45, max_num_hands=4) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, img = cap.read() # bgr\n",
    "        if img is None:\n",
    "            break\n",
    "        img = imutils.resize(img, width=resize_w)\n",
    "\n",
    "        # People Detection\n",
    "        results = people_detection_model(img, stream=True, verbose=False)\n",
    "        people_detections = np.empty((0, 5))\n",
    "        for r in results:  \n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                conf = math.ceil(box.conf[0] * 100) / 100\n",
    "                cls = classNames[int(box.cls[0])]\n",
    "                if cls == 'person' and conf > 0.3:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    people_detections = np.vstack((people_detections, np.array([x1, y1, x2, y2, conf])))\n",
    "                    \n",
    "        results_tracker = tracker.update(people_detections)\n",
    "        del people_detections\n",
    "\n",
    "        # Hand Detection\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        ### POSE DETECTION ###\n",
    "        func = lambda x: x if x.visibility > 0.4 else None\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                lm = hand.landmark\n",
    "                x1, y1 = int(min([i.x for i in lm]) * resize_w), int(min([i.y for i in lm]) * resize_h)\n",
    "                x2, y2 = int(max([i.x for i in lm]) * resize_w), int(max([i.y for i in lm]) * resize_h)\n",
    "                \n",
    "                current_hand_detection = np.array([x1, y1, x2, y2])\n",
    "                x1, y1 = max(x1-offset, 0), max(y1-offset, 0)\n",
    "                x2, y2 = min(x2+offset, resize_w), min(y2+offset, resize_h)\n",
    "\n",
    "                # check if holding handrails\n",
    "                hand_img = img[y1:y2+1, x1:x2+1, :]\n",
    "                hand_img = cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB)\n",
    "                # print(\"hand\")\n",
    "                # display(Image.fromarray(np.uint8(hand_img)))\n",
    "\n",
    "                inputs = processor(text=prompt_lst, images=Image.fromarray(hand_img), return_tensors=\"pt\")\n",
    "                \n",
    "                # ALIGN Prediction\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                logits_per_image = outputs.logits_per_image\n",
    "                probs = logits_per_image.softmax(dim=1)\n",
    "                \n",
    "                if probs[0][0] >= 0.9: \n",
    "                    # based on hand bounding box, find the corresponding people bounding box\n",
    "                    people_Id = find_people_bounding_box(results_tracker, current_hand_detection)\n",
    "                    if people_Id not in already_used:\n",
    "                        already_used.append(people_Id)\n",
    "        # When there is no hand detected in the current frame, we use pose estimation as the second checking mechanism\n",
    "        else:\n",
    "            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img.flags.writeable = False\n",
    "                results = holistic.process(img)\n",
    "                img.flags.writeable = True\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                hand_boxes = []\n",
    "    \n",
    "                if results.pose_landmarks:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    right = [func(landmarks[mp_pose.PoseLandmark.RIGHT_PINKY]), func(landmarks[mp_pose.PoseLandmark.RIGHT_INDEX]), func(landmarks[mp_pose.PoseLandmark.RIGHT_THUMB])]\n",
    "                    if None not in right:\n",
    "                        right_x1, right_y1 = int(min([i.x for i in right]) * resize_w), int(min([i.y for i in right]) * resize_h)\n",
    "                        right_x2, right_y2 = int(max([i.x for i in right]) * resize_w), int(max([i.y for i in right]) * resize_h)\n",
    "                        right_x1, right_y1 = max(right_x1-pose_offset, 0), max(right_y1-pose_offset, 0)\n",
    "                        right_x2, right_y2 = min(right_x2+pose_offset, resize_w), min(right_y2+pose_offset, resize_h)\n",
    "                        # cvzone.cornerRect(img, (right_x1, right_y1, right_x2-right_x1, right_y2-right_y1), l=5, rt=1, colorR=(255, 0, 255))\n",
    "                        hand_boxes.append((right_x1, right_x2, right_y1, right_y2))\n",
    "                    left = [func(landmarks[mp_pose.PoseLandmark.LEFT_THUMB]), func(landmarks[mp_pose.PoseLandmark.LEFT_PINKY]), func(landmarks[mp_pose.PoseLandmark.LEFT_INDEX])]\n",
    "                    if None not in left:\n",
    "                        left_x1, left_y1 = int(min([i.x for i in left]) * resize_w), int(min([i.y for i in left]) * resize_h)\n",
    "                        left_x2, left_y2 = int(max([i.x for i in left]) * resize_w), int(max([i.y for i in left]) * resize_h)\n",
    "                        left_x1, left_y1 = max(left_x1-pose_offset, 0), max(left_y1-pose_offset, 0)\n",
    "                        left_x2, left_y2 = min(left_x2+pose_offset, resize_w), min(left_y2+pose_offset, resize_h)\n",
    "                        # cvzone.cornerRect(img, (left_x1, left_y1, left_x2-left_x1, left_y2-left_y1), l=5, rt=1, colorR=(255, 0, 255))\n",
    "                        hand_boxes.append((left_x1, left_x2, left_y1, left_y2))\n",
    "                    for hand_box in hand_boxes:\n",
    "                        current_hand_detection = np.array(hand_box)\n",
    "                        x1, x2, y1, y2 = hand_box\n",
    "                        hand_img = img[y1:y2+1, x1:x2+1, :]\n",
    "                        try:\n",
    "                            hand_img = cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB)\n",
    "                            display(Image.fromarray(np.uint8(hand_img)))\n",
    "                        except:\n",
    "                            continue\n",
    "                        \n",
    "                        inputs = processor(text=prompt_lst, images=Image.fromarray(hand_img), return_tensors=\"pt\")\n",
    "                        \n",
    "                        # ALIGN Prediction\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(**inputs)\n",
    "                        \n",
    "                        logits_per_image = outputs.logits_per_image\n",
    "                        probs = logits_per_image.softmax(dim=1)\n",
    "                        print(probs[0][0])\n",
    "                        if probs[0][0] >= 0.9: \n",
    "                            # based on hand bounding box, find the corresponding people bounding box\n",
    "                            people_Id = find_people_bounding_box(results_tracker, current_hand_detection)\n",
    "                            if people_Id not in already_used:\n",
    "                                already_used.append(people_Id)\n",
    "\n",
    "        # draw bounding box around people\n",
    "        for r in results_tracker:\n",
    "            x1, y1, x2, y2, Id = r\n",
    "            x1, y1, x2, y2, Id = int(x1), int(y1), int(x2), int(y2), int(Id)\n",
    "            color = (0, 0, 255)\n",
    "            if Id in already_used:\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "            # Keep track of the number of frames each person has appeared in the video - sometimes we may have smaller bounding boxes that capture\n",
    "            # a part of the person's body (e.g. leg) because the leg appears before the whole body appears in the video.\n",
    "            if Id not in item_dict:\n",
    "                item_dict[Id] = 0\n",
    "            else:\n",
    "                item_dict[Id] += 1\n",
    "                if item_dict[Id] == 18:\n",
    "                    people_counter += 1\n",
    "                    \n",
    "            # cvzone.putTextRect(img, str(Id), (max(0, x1), max(35, y1)), scale=1, thickness=2, offset=5)\n",
    "            cvzone.cornerRect(img, (x1, y1, x2-x1, y2-y1), l=5, rt=2, colorR=color)\n",
    "\n",
    "        # display number of people holding the handrails\n",
    "        img = draw_counter(img, resize_w, resize_h, people_counter, len(set(already_used)))\n",
    "            \n",
    "        cv2.imshow('Hand Tracking', img)\n",
    "        counter += 1\n",
    "        output.write(img)\n",
    "        if cv2.waitKey(48) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    output.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8755f-f89a-4c73-a95a-145603e0b3ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OTHER IDEAS / TOOLS\n",
    "1. Replace ALIGN with NeVA (NVIDIA's Visual Question Answering Transformer) - requires API access. <br><br>\n",
    "<img src=\"./img/neva_cannot_see_hand.png\" width=400> <br><br>\n",
    "2. Detect handrail region -> image-to-text classification: use image segmentation models (e.g. Segment Anything Model (SAM)) to detect the regions where the handrails are (could use some common properties shared by different handrails, e.g. consistent width, length of the handrails (relative to people's size) and even height from the floor to determine whether the identified image segments represent handrail (However, difficult to prove if the assumptions are generalizable).<br><br>\n",
    "<img src=\"./img/handrail_only.png\" width=400> <br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
