{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe0163d5-4b0c-4d72-b690-09c10599777a",
   "metadata": {},
   "source": [
    "# Avatar Redaction/Replacement - Identify and redact avatars and fill in their space with environment context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c7042-c760-4ef7-b97a-6e84aee9c7e9",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "1. Avatar segmentation using a Pretrained FCN-ResNet model on DeepLabV3 architecture to remove avatar completely from footage.\n",
    "2. Employ inpainting which uses a Two-stage adversarial model EdgeConnect to fill in the space which the avatars body has used.\n",
    "3. Footage is converted into a sequence of frames and and the model is segmentation and inpainting models are applied to each frame.\n",
    "4. Frames are stitched back together and saved to a local directory.\n",
    "5. Depending if the user chooses to employ either CPU or GPU, the codebase will produce a lower-resolution video or a higher resolution video respectively "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ae0eb-89f3-4126-a11d-f94a1044f55a",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836dc0c8-a824-4511-ad0d-a1704f7b98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "from src.config import Config\n",
    "from src.edge_connect import EdgeConnect\n",
    "from argparse import Namespace\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import glob\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "from skimage.feature import canny\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "from src.utils import create_mask\n",
    "from src.segmentor_fcn import segmentor,fill_gaps\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models import EdgeModel, InpaintingModel\n",
    "from src.utils import Progbar, create_dir, stitch_images, imsave\n",
    "\n",
    "import cv2\n",
    "from cv2 import dnn_superres\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0adb5-57b8-4b80-ba07-b09212db75fa",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09db3a01-4beb-4436-8d6d-1a35c2db815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(modelType = None, res = None, folder = None):\n",
    "    r\"\"\"starts the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config = load_config(modelType, res, folder)\n",
    "    \n",
    "    # cuda visble devices\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(e) for e in config.GPU)\n",
    "\n",
    "\n",
    "    # init device\n",
    "    if torch.cuda.is_available():\n",
    "        config.DEVICE = torch.device(\"cuda\")\n",
    "        torch.backends.cudnn.benchmark = True   # cudnn auto-tuner\n",
    "    else:\n",
    "        config.DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    # set cv2 running threads to 1 (prevents deadlocks with pytorch dataloader)\n",
    "    cv2.setNumThreads(0)\n",
    "\n",
    "    # initialize random seed\n",
    "    torch.manual_seed(config.SEED)\n",
    "    torch.cuda.manual_seed_all(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    random.seed(config.SEED)\n",
    "\n",
    "    # build the model and initialize\n",
    "    model = EdgeConnect(config)\n",
    "\n",
    "    model.load()\n",
    "\n",
    "    # model test\n",
    "    print('begin redaction...\\n')\n",
    "    model.test(folder)\n",
    "\n",
    "    ######################## remove input / output folders ###################################\n",
    "    inputPath = 'examples/input/' + folder\n",
    "    inputPath = os.path.join(os.getcwd(), inputPath)\n",
    "    print(\"Removing \" + inputPath + \"...\")\n",
    "    shutil.rmtree(inputPath)\n",
    "    \n",
    "    \n",
    "    outputPath = 'examples/output/' + folder\n",
    "    outputPath = os.path.join(os.getcwd(), outputPath)\n",
    "    print(\"Removing \" + outputPath + \"...\")\n",
    "    shutil.rmtree(outputPath)\n",
    "    ######################## remove input / output folders ###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d8f59-f45f-405a-a66e-e5f82bf8e482",
   "metadata": {},
   "source": [
    "## Load Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048a69ad-c808-41c8-9909-29ba3bd2e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(modelType=None, res = None, folder = None):\n",
    "    r\"\"\"loads model config\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # cpu mode\n",
    "    if res == \"cpu\":\n",
    "        config_dict = {\n",
    "            \"cpu\": 'yes', \n",
    "            \"edge\": None, \n",
    "            \"input\": './examples/input/' + folder, \n",
    "            \"model\": modelType, \n",
    "            \"output\": './examples/output/' + folder, \n",
    "            \"path\": './checkpoints', \n",
    "            \"remove\": [15]\n",
    "                   }\n",
    "        args = Namespace(**config_dict)\n",
    "        print(args)\n",
    "\n",
    "    # gpu mode\n",
    "    elif res == \"gpu\":\n",
    "        config_dict = {\n",
    "            \"cpu\": None, \n",
    "            \"edge\": None, \n",
    "            \"input\": './examples/input/' + folder, \n",
    "            \"model\": modelType, \n",
    "            \"output\": './examples/output/' + folder, \n",
    "            \"path\": './checkpoints', \n",
    "            \"remove\": [15],\n",
    "                    }\n",
    "        args = Namespace(**config_dict)\n",
    "        \n",
    "        print(\"Machine Configuration\")\n",
    "        print(args, \"\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(\"please input either cpu or gpu as your mode\")   \n",
    "\n",
    "    ######################## create frames ########################\n",
    "    vidcap = cv2.VideoCapture('./examples/source/' + folder + '.mp4')\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    inputPath = 'examples/input/' + folder\n",
    "    inputPath = os.path.join(os.getcwd(), inputPath)\n",
    "    try:  \n",
    "        os.mkdir(inputPath)  \n",
    "    except OSError as error:  \n",
    "        print(\"path already exists\") \n",
    "    while success:\n",
    "      cv2.imwrite('./examples/input/' + folder + '/' + \"frame%07d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "      success,image = vidcap.read()\n",
    "      count += 1\n",
    "    ######################## create frames ########################\n",
    "    \n",
    "    #if path for checkpoint not given\n",
    "    if args.path is None:\n",
    "        args.path='./checkpoints'\n",
    "    config_path = os.path.join(args.path, 'config.yml')\n",
    "    \n",
    "       # create checkpoints path if does't exist\n",
    "    if not os.path.exists(args.path):\n",
    "        os.makedirs(args.path)\n",
    "\n",
    "    # copy config template if does't exist\n",
    "    if not os.path.exists(config_path):\n",
    "        copyfile('./config.yml.example', config_path)\n",
    "\n",
    "    # load config file\n",
    "    config = Config(config_path)\n",
    "   \n",
    "    # eval mode\n",
    "    config.MODE = 3 # 1 train, 2 test, 3 eval\n",
    "    config.MODEL = args.model if args.model is not None else 3\n",
    "    config.OBJECTS = args.remove if args.remove is not None else [15]\n",
    "    config.SEG_DEVICE = 'cpu' if args.cpu is not None else 'cuda'\n",
    "    config.INPUT_SIZE = 256\n",
    "\n",
    "    # inpout PATH\n",
    "    if args.input is not None:\n",
    "        config.TEST_FLIST = args.input\n",
    "    \n",
    "    if args.edge is not None:\n",
    "        config.TEST_EDGE_FLIST = args.edge\n",
    "\n",
    "    # output PATH\n",
    "    if args.output is not None:\n",
    "        config.RESULTS = args.output\n",
    "    else: \n",
    "        if not os.path.exists('./results_images'):\n",
    "            os.makedirs('./results_images')\n",
    "        config.RESULTS = './results_images'\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075cb30-c51f-4b9b-a23e-3439f0ddea3a",
   "metadata": {},
   "source": [
    "## Split Video into a sequence of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5cdbca-f6fa-48b5-873f-7bf4cbf1e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, config, flist, edge_flist, augment=True, training=True):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.augment = augment\n",
    "        self.training = training\n",
    "        self.data = self.load_flist(flist)\n",
    "        self.edge_data = self.load_flist(edge_flist)\n",
    "\n",
    "        self.input_size = config.INPUT_SIZE\n",
    "        self.sigma = config.SIGMA\n",
    "        self.edge = config.EDGE\n",
    "        self.mask = config.MASK\n",
    "        self.nms = config.NMS\n",
    "        self.device = config.SEG_DEVICE\n",
    "        self.objects = config.OBJECTS\n",
    "        self.segment_net = config.SEG_NETWORK\n",
    "        # in test mode, there's a one-to-one relationship between mask and image\n",
    "        # masks are loaded non random\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            item = self.load_item(index)\n",
    "        except:\n",
    "            print('loading error: ' + self.data[index])\n",
    "            item = self.load_item(0)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def load_name(self, index):\n",
    "        name = self.data[index]\n",
    "        return os.path.basename(name)\n",
    "        \n",
    "    def load_size(self, index):\n",
    "        img = Image.open(self.data[index])\n",
    "        width,height=img.size\n",
    "        return width,height\n",
    "\n",
    "\n",
    "    def load_item(self, index):\n",
    "\n",
    "        size = self.input_size\n",
    "\n",
    "        # load image\n",
    "        img = Image.open(self.data[index])\n",
    "        \n",
    "        \n",
    "        # gray to rgb\n",
    "        if img.mode !='RGB':\n",
    "            img = gray2rgb(np.array(img))\n",
    "            img=Image.fromarray(img)\n",
    "\n",
    "        # resize/crop if needed\n",
    "        img,mask=segmentor(self.segment_net,img,self.device,self.objects)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        # # print(\"show image from dataset before resize\")\n",
    "        # plt.imshow(img); plt.show()\n",
    "\n",
    "\n",
    "        ######################### Determine aspect ratio ######################  \n",
    "        # find aspect ratio\n",
    "        width_og, height_og = img.size\n",
    "        ratio = (width_og/height_og)\n",
    "\n",
    "        if (ratio < 1):    \n",
    "            size_W = int(size)\n",
    "            size_H = int(452)      \n",
    "        elif (ratio > 1):\n",
    "            size_W = int(452)\n",
    "            size_H = int(size)\n",
    "        else:\n",
    "            size_W = (size)\n",
    "            size_H = (size)\n",
    "        ######################### Determine aspect ratio ######################  \n",
    "        \n",
    "        # resize to square image\n",
    "        img = np.array(img.resize((size_W, size_H), Image.LANCZOS))\n",
    "\n",
    "        # print(\"show image from dataset after resize\")\n",
    "        # plt.imshow(img); plt.show()\n",
    "\n",
    "        # create grayscale image\n",
    "        img_gray = rgb2gray(np.array(img))\n",
    "\n",
    "        # load mask\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        # resize to square image\n",
    "        mask = np.array(mask.resize((size_W, size_H), Image.LANCZOS))\n",
    "\n",
    "        # # print(\"show mask from dataset\")\n",
    "        # plt.imshow(mask); plt.show()\n",
    "\n",
    "        idx=(mask>0)\n",
    "        mask[idx]=255\n",
    "        #kernel = np.ones((5, 5), np.uint8)\n",
    "        #opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        #closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "        mask=np.apply_along_axis(fill_gaps, 1, mask) #horizontal padding\n",
    "        mask=np.apply_along_axis(fill_gaps, 0, mask) #vertical padding\n",
    "        \n",
    "        # load edge\n",
    "        edge = self.load_edge(img_gray, index, mask)\n",
    "\n",
    "        # augment data\n",
    "        if self.augment and np.random.binomial(1, 0.5) > 0:\n",
    "            img = img[:, ::-1, ...]\n",
    "            img_gray = img_gray[:, ::-1, ...]\n",
    "            edge = edge[:, ::-1, ...]\n",
    "            mask = mask[:, ::-1, ...]\n",
    "\n",
    "        return self.to_tensor(img), self.to_tensor(img_gray), self.to_tensor(edge), self.to_tensor(mask)\n",
    "\n",
    "    def load_edge(self, img, index, mask):\n",
    "        sigma = self.sigma\n",
    "\n",
    "        # in test mode images are masked (with masked regions),\n",
    "        # using 'mask' parameter prevents canny to detect edges for the masked regions\n",
    "        mask = None if self.training else (1 - mask / 255).astype(np.bool)\n",
    "        \n",
    "        # canny\n",
    "        if self.edge == 1:\n",
    "            # no edge\n",
    "            if sigma == -1:\n",
    "                return np.zeros(img.shape).astype(np.float)\n",
    "\n",
    "            # random sigma\n",
    "            if sigma == 0:\n",
    "                sigma = random.randint(1, 4)\n",
    "\n",
    "            return canny(img, sigma=sigma, mask=mask).astype(np.float)\n",
    "\n",
    "        # external\n",
    "        else:\n",
    "            imgh, imgw = img.shape[0:2]\n",
    "            edge = imread(self.edge_data[index])\n",
    "            edge = self.resized(edge, imgh, imgw)\n",
    "\n",
    "            # non-max suppression\n",
    "            if self.nms == 1:\n",
    "                edge = edge * canny(img, sigma=sigma, mask=mask)\n",
    "\n",
    "            return edge\n",
    "\n",
    "    \n",
    "    def to_tensor(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img_t = F.to_tensor(img).float()\n",
    "        return img_t\n",
    "\n",
    "\n",
    "    def load_flist(self, flist):\n",
    "        if isinstance(flist, list):\n",
    "            return flist\n",
    "\n",
    "        # flist: image file path, image directory path, text file flist path\n",
    "        if isinstance(flist, str):\n",
    "            if os.path.isdir(flist):\n",
    "                flist = list(glob.glob(flist + '/*.jpg')) + list(glob.glob(flist + '/*.png'))\n",
    "                flist.sort()\n",
    "                return flist\n",
    "\n",
    "            if os.path.isfile(flist):\n",
    "                try:\n",
    "                    return np.genfromtxt(flist, dtype=np.str, encoding='utf-8')\n",
    "                except:\n",
    "                    return [flist]\n",
    "\n",
    "        return []\n",
    "\n",
    "    def create_iterator(self, batch_size):\n",
    "        while True:\n",
    "            sample_loader = DataLoader(\n",
    "                dataset=self,\n",
    "                batch_size=batch_size,\n",
    "                drop_last=True\n",
    "            )\n",
    "\n",
    "            for item in sample_loader:\n",
    "                yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5fb4c7-765c-4f65-99ff-12a5d5ca1d53",
   "metadata": {},
   "source": [
    "## Apply model to each frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7858bf9-5018-4b80-8d3c-6e011fb7a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConnect():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        if config.MODEL == 1:\n",
    "            model_name = 'edge'\n",
    "        elif config.MODEL == 2:\n",
    "            model_name = 'inpaint'\n",
    "        elif config.MODEL == 3:\n",
    "            model_name = 'edge_inpaint'\n",
    "        elif config.MODEL == 4:\n",
    "            model_name = 'joint'\n",
    "\n",
    "        self.debug = False\n",
    "        self.model_name = model_name\n",
    "        self.edge_model = EdgeModel(config).to(config.DEVICE)\n",
    "        self.inpaint_model = InpaintingModel(config).to(config.DEVICE)\n",
    "\n",
    "        # test mode\n",
    "        self.test_dataset = Dataset(config, config.TEST_FLIST, config.TEST_EDGE_FLIST, augment=False, training=False)\n",
    "      \n",
    "\n",
    "        self.samples_path = os.path.join(config.PATH, 'samples')\n",
    "        \n",
    "        self.results_path = os.path.join(config.PATH, 'results')\n",
    "\n",
    "        if config.RESULTS is not None:\n",
    "            self.results_path = os.path.join(config.RESULTS)\n",
    "\n",
    "        if config.DEBUG is not None and config.DEBUG != 0:\n",
    "            self.debug = True\n",
    "\n",
    "        self.log_file = os.path.join(config.PATH, 'log_' + model_name + '.dat')\n",
    "\n",
    "    def load(self):\n",
    "        if self.config.MODEL == 1:\n",
    "            self.edge_model.load()\n",
    "\n",
    "        elif self.config.MODEL == 2:\n",
    "            self.inpaint_model.load()\n",
    "\n",
    "        else:\n",
    "            self.edge_model.load()\n",
    "            self.inpaint_model.load()\n",
    "\n",
    "    def save(self):\n",
    "        if self.config.MODEL == 1:\n",
    "            self.edge_model.save()\n",
    "\n",
    "        elif self.config.MODEL == 2 or self.config.MODEL == 3:\n",
    "            self.inpaint_model.save()\n",
    "\n",
    "        else:\n",
    "            self.edge_model.save()\n",
    "            self.inpaint_model.save()\n",
    "\n",
    "\n",
    "    def test(self, folder):\n",
    "        self.edge_model.eval()\n",
    "        self.inpaint_model.eval()\n",
    "\n",
    "        model = self.config.MODEL\n",
    "        create_dir(self.results_path)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=self.test_dataset,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "\n",
    "         ######################### upscale for GPU use ######################   \n",
    "        if (self.config.SEG_DEVICE != \"cpu\"):\n",
    "\n",
    "            # Create an SR object\n",
    "            sr = dnn_superres.DnnSuperResImpl_create()\n",
    "         ######################### upscale for GPU use ######################  \n",
    "        \n",
    "        index = 0\n",
    "        for items in test_loader:        \n",
    "            name = self.test_dataset.load_name(index)\n",
    "            \n",
    "            images, images_gray, edges, masks = self.cuda(*items)\n",
    "            index += 1\n",
    "\n",
    "            # edge model\n",
    "            if model == 1:\n",
    "                outputs = self.edge_model(images_gray, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (edges * (1 - masks))\n",
    "\n",
    "            # inpaint model\n",
    "            elif model == 2:\n",
    "                outputs = self.inpaint_model(images, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "            # inpaint with edge model / joint model\n",
    "            else:\n",
    "                edges = self.edge_model(images_gray, edges, masks).detach()\n",
    "                outputs = self.inpaint_model(images, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "            output = self.postprocess(outputs_merged)[0]\n",
    "\n",
    "            # print(\"load original image and get size\")\n",
    "            img = Image.open(self.test_dataset.__dict__['data'][0])\n",
    "            # print(self.test_dataset.__dict__['data'][0])\n",
    "            width_og, height_og = img.size\n",
    "\n",
    "            path = os.path.join(self.results_path, name)\n",
    "            print(index, name)\n",
    "            imsave(output, path)\n",
    "            \n",
    "            # print(path)\n",
    "            \n",
    "            ######################### upscale for GPU use ######################  \n",
    "            if (self.config.SEG_DEVICE != \"cpu\"):\n",
    "                # Read image\n",
    "                image_sr = cv2.imread(path)\n",
    "                # Read the desired model\n",
    "                # model_sr = \"EDSR_x3.pb\"\n",
    "                model_sr = \"FSRCNN_x4.pb\"\n",
    "                sr.readModel(model_sr)\n",
    "                # Set the desired model and scale to get correct pre- and post-processing\n",
    "                sr.setModel(\"fsrcnn\", 4)\n",
    "                # Upscale the image\n",
    "                result_sr = sr.upsample(image_sr)\n",
    "                # Save the image\n",
    "                cv2.imwrite(path, result_sr)\n",
    "            ######################### upscale for GPU use ######################  \n",
    "\n",
    "            if self.debug:\n",
    "                edges = self.postprocess(1 - edges)[0]\n",
    "                masked = self.postprocess(images * (1 - masks) + masks)[0]\n",
    "                fname, fext = name.split('.')\n",
    "\n",
    "                imsave(edges, os.path.join(self.results_path, fname + '_edge.' + fext))\n",
    "                imsave(masked, os.path.join(self.results_path, fname + '_masked.' + fext))\n",
    "\n",
    "        ########################## create output video ##############################\n",
    "        outDirectory = 'examples/output/'\n",
    "        \n",
    "        outPath = outDirectory + folder + '/*.jpg'\n",
    "        outPath = os.path.join(os.getcwd(), outPath)\n",
    "        \n",
    "        img = Image.open('./' + outDirectory + folder + '/' + name)\n",
    "        \n",
    "        width_og, height_og = img.size\n",
    "        \n",
    "        img_array = []\n",
    "        \n",
    "        for filename in glob.glob(outPath):\n",
    "            img = cv2.imread(filename)\n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            img_array.append(img)\n",
    "        out = cv2.VideoWriter(outDirectory + ('output-' + folder + '.mp4'),cv2.VideoWriter_fourcc(*'MP4V'), 30, size)\n",
    "        for i in range(len(img_array)):\n",
    "            out.write(img_array[i])\n",
    "        out.release()\n",
    "        ########################## create output video ##############################\n",
    "        \n",
    "        print('\\nEnd redaction....')\n",
    "        return output\n",
    "\n",
    "\n",
    "    def log(self, logs):\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write('%s\\n' % ' '.join([str(item[1]) for item in logs]))\n",
    "\n",
    "    def cuda(self, *args):\n",
    "        return (item.to(self.config.DEVICE) for item in args)\n",
    "\n",
    "    def postprocess(self, img):\n",
    "        # [0, 1] => [0, 255]\n",
    "        img = img * 255.0\n",
    "        img = img.permute(0, 2, 3, 1)\n",
    "        return img.int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaab238-68bd-4d48-b1ea-2a4e1b8a310b",
   "metadata": {},
   "source": [
    "Model is ran using the main function which can be configuered to run on CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d5acf-252b-49ae-94b5-9e8ccb93ebcb",
   "metadata": {},
   "source": [
    "# Parameters: main(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2425a4-49e0-43b1-82db-edcd30905d43",
   "metadata": {},
   "source": [
    "#### <strong>x</strong> : The modol will be applied to the video - <strong> 1 </strong> : <em> edge model </em> ,  <strong> 2 </strong> : <em> inpaint model </em>,  <strong> 3 </strong> : <em> edge-inpaint model </em>,  <strong> 4 </strong> : <em> joint model </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8464a-f953-474d-b647-86372f8d14b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <strong>y</strong> : Type of machine which the redation is being performed - <strong> cpu </strong> : <em> CPU mode </em> (low freame-rate, low-resolution video), <strong> gpu </strong> : <em> GPU mode </em> (full frame-rate, full resolution via upscaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78ba3e-b76a-41e1-ac11-ef487d917b10",
   "metadata": {},
   "source": [
    "#### <strong>z</strong> : Name of source video - Source video will need to be stored in the <strong> \\examples\\source\\ </strong> folder within the repositry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48dfd5-db51-499b-bf50-6367f8ef77c3",
   "metadata": {},
   "source": [
    "Once the model has completed redaction / inpainting, then the output video will be stored in <strong> \\examples\\output\\ </strong>, sharing the same name as the source video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53594b8-4ec6-4235-94ed-cbb37fe298cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cpu='yes', edge=None, input='./examples/input/shopping2', model=3, output='./examples/output/shopping2', path='./checkpoints', remove=[15])\n",
      "Loading EdgeModel generator...\n",
      "Loading InpaintingModel generator...\n",
      "begin redaction...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gomolina\\Anaconda3\\envs\\objRemover\\lib\\site-packages\\torchvision\\transforms\\functional.py:74: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "C:\\Users\\gomolina\\Anaconda3\\envs\\objRemover\\lib\\site-packages\\torchvision\\transforms\\functional.py:70: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 frame0000000.jpg\n",
      "2 frame0000001.jpg\n",
      "3 frame0000002.jpg\n",
      "4 frame0000003.jpg\n",
      "5 frame0000004.jpg\n",
      "6 frame0000005.jpg\n",
      "7 frame0000006.jpg\n",
      "8 frame0000007.jpg\n",
      "9 frame0000008.jpg\n",
      "10 frame0000009.jpg\n",
      "11 frame0000010.jpg\n",
      "12 frame0000011.jpg\n",
      "13 frame0000012.jpg\n",
      "14 frame0000013.jpg\n",
      "15 frame0000014.jpg\n",
      "16 frame0000015.jpg\n",
      "17 frame0000016.jpg\n",
      "18 frame0000017.jpg\n",
      "19 frame0000018.jpg\n",
      "20 frame0000019.jpg\n",
      "21 frame0000020.jpg\n",
      "22 frame0000021.jpg\n",
      "23 frame0000022.jpg\n",
      "24 frame0000023.jpg\n",
      "25 frame0000024.jpg\n",
      "26 frame0000025.jpg\n",
      "27 frame0000026.jpg\n",
      "28 frame0000027.jpg\n",
      "29 frame0000028.jpg\n",
      "30 frame0000029.jpg\n",
      "31 frame0000030.jpg\n",
      "32 frame0000031.jpg\n",
      "33 frame0000032.jpg\n",
      "34 frame0000033.jpg\n",
      "35 frame0000034.jpg\n",
      "36 frame0000035.jpg\n",
      "37 frame0000036.jpg\n",
      "38 frame0000037.jpg\n",
      "39 frame0000038.jpg\n",
      "40 frame0000039.jpg\n",
      "41 frame0000040.jpg\n",
      "42 frame0000041.jpg\n",
      "43 frame0000042.jpg\n",
      "44 frame0000043.jpg\n",
      "45 frame0000044.jpg\n",
      "46 frame0000045.jpg\n",
      "47 frame0000046.jpg\n",
      "48 frame0000047.jpg\n",
      "49 frame0000048.jpg\n",
      "50 frame0000049.jpg\n",
      "51 frame0000050.jpg\n",
      "52 frame0000051.jpg\n",
      "53 frame0000052.jpg\n",
      "54 frame0000053.jpg\n",
      "55 frame0000054.jpg\n",
      "56 frame0000055.jpg\n",
      "57 frame0000056.jpg\n",
      "58 frame0000057.jpg\n",
      "59 frame0000058.jpg\n",
      "60 frame0000059.jpg\n",
      "61 frame0000060.jpg\n",
      "62 frame0000061.jpg\n",
      "63 frame0000062.jpg\n",
      "64 frame0000063.jpg\n",
      "65 frame0000064.jpg\n",
      "66 frame0000065.jpg\n",
      "67 frame0000066.jpg\n",
      "68 frame0000067.jpg\n",
      "69 frame0000068.jpg\n",
      "70 frame0000069.jpg\n",
      "71 frame0000070.jpg\n",
      "72 frame0000071.jpg\n",
      "73 frame0000072.jpg\n",
      "74 frame0000073.jpg\n",
      "75 frame0000074.jpg\n",
      "76 frame0000075.jpg\n",
      "77 frame0000076.jpg\n",
      "78 frame0000077.jpg\n",
      "79 frame0000078.jpg\n",
      "80 frame0000079.jpg\n",
      "81 frame0000080.jpg\n",
      "82 frame0000081.jpg\n",
      "83 frame0000082.jpg\n",
      "84 frame0000083.jpg\n",
      "85 frame0000084.jpg\n",
      "86 frame0000085.jpg\n",
      "87 frame0000086.jpg\n",
      "88 frame0000087.jpg\n",
      "89 frame0000088.jpg\n",
      "90 frame0000089.jpg\n",
      "91 frame0000090.jpg\n",
      "92 frame0000091.jpg\n",
      "93 frame0000092.jpg\n",
      "94 frame0000093.jpg\n",
      "95 frame0000094.jpg\n",
      "96 frame0000095.jpg\n",
      "97 frame0000096.jpg\n",
      "98 frame0000097.jpg\n",
      "99 frame0000098.jpg\n",
      "100 frame0000099.jpg\n",
      "101 frame0000100.jpg\n",
      "102 frame0000101.jpg\n",
      "103 frame0000102.jpg\n",
      "104 frame0000103.jpg\n",
      "105 frame0000104.jpg\n",
      "106 frame0000105.jpg\n",
      "107 frame0000106.jpg\n",
      "108 frame0000107.jpg\n",
      "109 frame0000108.jpg\n",
      "110 frame0000109.jpg\n",
      "111 frame0000110.jpg\n",
      "112 frame0000111.jpg\n",
      "113 frame0000112.jpg\n",
      "114 frame0000113.jpg\n",
      "115 frame0000114.jpg\n",
      "116 frame0000115.jpg\n",
      "117 frame0000116.jpg\n",
      "118 frame0000117.jpg\n",
      "119 frame0000118.jpg\n",
      "120 frame0000119.jpg\n",
      "121 frame0000120.jpg\n",
      "122 frame0000121.jpg\n",
      "123 frame0000122.jpg\n",
      "124 frame0000123.jpg\n",
      "125 frame0000124.jpg\n",
      "126 frame0000125.jpg\n",
      "127 frame0000126.jpg\n",
      "128 frame0000127.jpg\n",
      "129 frame0000128.jpg\n",
      "130 frame0000129.jpg\n",
      "131 frame0000130.jpg\n",
      "132 frame0000131.jpg\n",
      "133 frame0000132.jpg\n",
      "134 frame0000133.jpg\n",
      "135 frame0000134.jpg\n",
      "136 frame0000135.jpg\n",
      "137 frame0000136.jpg\n",
      "138 frame0000137.jpg\n",
      "139 frame0000138.jpg\n",
      "140 frame0000139.jpg\n",
      "141 frame0000140.jpg\n",
      "142 frame0000141.jpg\n",
      "143 frame0000142.jpg\n",
      "144 frame0000143.jpg\n",
      "145 frame0000144.jpg\n",
      "146 frame0000145.jpg\n",
      "147 frame0000146.jpg\n",
      "148 frame0000147.jpg\n",
      "149 frame0000148.jpg\n",
      "150 frame0000149.jpg\n",
      "151 frame0000150.jpg\n",
      "152 frame0000151.jpg\n",
      "153 frame0000152.jpg\n",
      "154 frame0000153.jpg\n",
      "155 frame0000154.jpg\n",
      "156 frame0000155.jpg\n",
      "157 frame0000156.jpg\n",
      "158 frame0000157.jpg\n",
      "159 frame0000158.jpg\n",
      "160 frame0000159.jpg\n",
      "161 frame0000160.jpg\n",
      "162 frame0000161.jpg\n",
      "163 frame0000162.jpg\n",
      "164 frame0000163.jpg\n",
      "165 frame0000164.jpg\n",
      "166 frame0000165.jpg\n",
      "167 frame0000166.jpg\n",
      "168 frame0000167.jpg\n",
      "169 frame0000168.jpg\n",
      "170 frame0000169.jpg\n",
      "171 frame0000170.jpg\n",
      "172 frame0000171.jpg\n",
      "173 frame0000172.jpg\n",
      "174 frame0000173.jpg\n",
      "175 frame0000174.jpg\n",
      "176 frame0000175.jpg\n",
      "177 frame0000176.jpg\n",
      "178 frame0000177.jpg\n",
      "179 frame0000178.jpg\n",
      "180 frame0000179.jpg\n",
      "181 frame0000180.jpg\n",
      "182 frame0000181.jpg\n",
      "183 frame0000182.jpg\n",
      "184 frame0000183.jpg\n",
      "185 frame0000184.jpg\n",
      "186 frame0000185.jpg\n",
      "187 frame0000186.jpg\n",
      "188 frame0000187.jpg\n",
      "189 frame0000188.jpg\n",
      "190 frame0000189.jpg\n",
      "191 frame0000190.jpg\n",
      "192 frame0000191.jpg\n",
      "193 frame0000192.jpg\n",
      "194 frame0000193.jpg\n",
      "195 frame0000194.jpg\n",
      "196 frame0000195.jpg\n",
      "197 frame0000196.jpg\n",
      "198 frame0000197.jpg\n",
      "199 frame0000198.jpg\n",
      "200 frame0000199.jpg\n",
      "201 frame0000200.jpg\n",
      "202 frame0000201.jpg\n",
      "203 frame0000202.jpg\n",
      "204 frame0000203.jpg\n",
      "205 frame0000204.jpg\n",
      "\n",
      "End redaction....\n",
      "Removing C:\\Users\\gomolina\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Automated-objects-removal-inpainter\\examples/input/shopping2...\n",
      "Removing C:\\Users\\gomolina\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Automated-objects-removal-inpainter\\examples/output/shopping2...\n"
     ]
    }
   ],
   "source": [
    "# the following function passes three arguments to the main function\n",
    "# 3 : an edge-inpaint model will be applied to the video\n",
    "# cpu : a cpu friendly version of the codebase will be ran on the video input - this will result in a slower inference time and lower-resolution output video\n",
    "# shopping2 : shopping2 is the name of the input file located in the project directory \\examples\\source\\shopping2.mp4\n",
    "\n",
    "main(3, \"cpu\", \"shopping2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19fa03-5f57-4a02-ad2c-cd2163bcf860",
   "metadata": {},
   "source": [
    "#### The below footage was passed through the model in GPU mode. I.e: main(3, \"cpu\", \"shopping2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f01514-6548-467f-b9db-c68758b68834",
   "metadata": {},
   "source": [
    "<video width=\"426\" src=\"examples\\source\\shopping2.mp4\" controls title=\"Title\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef56d2-55af-475d-81ce-97c3843d2978",
   "metadata": {},
   "source": [
    "#### The below footage is the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef127cce-42be-4698-aa07-6bcfa690afcc",
   "metadata": {},
   "source": [
    "<video width=\"426\" src=\"examples\\output\\output-shopping2.mp4\" controls title=\"Title\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b0e51-2211-44e1-920c-fb182e4a78d2",
   "metadata": {},
   "source": [
    "### Here are both video put together to run simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ff9e2-0fa0-4db2-8096-6899e71707f5",
   "metadata": {},
   "source": [
    "<video width=\"426\" src=\"examples\\demo\\wareHouse.mp4\" controls title=\"Title\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b6a3c-8f32-4379-bb36-025bee836772",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21245e3e-6f6c-45bf-adb3-9431d9ee9bdf",
   "metadata": {},
   "source": [
    "#### 1 : Source pre-trained models to include indoor door settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cc943-7e9e-40a8-a101-618344420a7d",
   "metadata": {},
   "source": [
    "\r\n",
    "The pretrained model performs at itâ€™s weakest when encountering indoor / irregular camera angled settings. Sourcing pre-trained models or training a model with labelled cctv footage is ideal so it best handles those environments when encountered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95720b1-fb8b-49cb-8e90-574eafb330d6",
   "metadata": {},
   "source": [
    "#### 2 : Ability to redact specific avatars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0a4cd-0f3e-47b1-9795-302b572f4ef8",
   "metadata": {},
   "source": [
    "\r\n",
    "Consenting parties may want to appear in the video footage once the film had pasted through the model. \r\n",
    "\r\n",
    "Solution can be configured to only redact figures who are not wearing high-vis jackets (employee of a work site) and blur everyone else (general public). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf6244-6e07-4e67-b12e-1a72d7ed79d0",
   "metadata": {},
   "source": [
    "#### 3 : Ability to redact specific avatars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e17ff-dfb0-48e7-bc50-7bdfc2b68a59",
   "metadata": {},
   "source": [
    "Feed results of the previous frame into the next frame to smooth out video \n",
    "\r\n",
    "The video output can sometimes look jittery due to applying the inpainting model to each frame individually. \r\n",
    "\r\n",
    "A way to enhance the output video would be to use the result of an impainted frame (the model has redacted the avatar and filled in the gap with environment context) and use that filled in frame as context for the next framvideo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d7ef40-8baf-4d28-b814-36f706d09364",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './examples/output/shopping2/frame0000204.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m outPath \u001b[38;5;241m=\u001b[39m outDirectory \u001b[38;5;241m+\u001b[39m folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m outPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), outPath)\n\u001b[1;32m----> 9\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutDirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m width_og, height_og \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     13\u001b[0m img_array \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\objRemover\\lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './examples/output/shopping2/frame0000204.jpg'"
     ]
    }
   ],
   "source": [
    "folder = \"shopping2\"\n",
    "name = \"frame0000204.jpg\"\n",
    "\n",
    "outDirectory = 'examples/output/'\n",
    "\n",
    "outPath = outDirectory + folder + '/*.jpg'\n",
    "outPath = os.path.join(os.getcwd(), outPath)\n",
    "\n",
    "img = Image.open('./' + outDirectory + folder + '/' + name)\n",
    "\n",
    "width_og, height_og = img.size\n",
    "\n",
    "img_array = []\n",
    "\n",
    "for filename in glob.glob(outPath):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "out = cv2.VideoWriter(outDirectory + ('output-' + folder + '.mp4'),cv2.VideoWriter_fourcc(*'MP4V'), 30, size)\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a758c3e-0b78-4d09-8097-ce0e1171954a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
