{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe0163d5-4b0c-4d72-b690-09c10599777a",
   "metadata": {},
   "source": [
    "# Avatar Redaction/Replacement - Identify and redact avatars and fill in their space with environment context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c7042-c760-4ef7-b97a-6e84aee9c7e9",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "1. Avatar segmentation using a Pretrained FCN-ResNet model on DeepLabV3 architecture to remove avatar completely from footage.\n",
    "2. Employ inpainting which uses a Two-stage adversarial model EdgeConnect to fill in the space which the avatars body has used.\n",
    "3. Footage is converted into a sequence of frames and and the model is segmentation and inpainting models are applied to each frame.\n",
    "4. Frames are stitched back together and saved to a local directory.\n",
    "5. Depending if the user chooses to employ either CPU or GPU, the codebase will produce a lower-resolution video or a higher resolution video respectively\n",
    "6. CPU will require long inference time, and is significantly slower than running on GPU. Model implementation is computationally demanding and overnight process of videos spanning a few minutes is requiered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87677ffb-e8d6-4c1e-a95b-5eb0a992274f",
   "metadata": {},
   "source": [
    "#### For installation instructions, please follow README after first initiating an environment via <em> anaconda </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159aa82-e142-4eb1-a784-5a5d5d12425d",
   "metadata": {},
   "source": [
    "##### The below code is a modification of https://github.com/sujaykhandekar/Automated-objects-removal-inpainter for the purposes of removing human avatars from a video feed (mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ae0eb-89f3-4126-a11d-f94a1044f55a",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836dc0c8-a824-4511-ad0d-a1704f7b98c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "from src.config import Config\n",
    "from src.edge_connect import EdgeConnect\n",
    "from argparse import Namespace\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import glob\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "from skimage.feature import canny\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "from src.utils import create_mask\n",
    "from src.segmentor_fcn import segmentor,fill_gaps\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models import EdgeModel, InpaintingModel\n",
    "from src.utils import Progbar, create_dir, stitch_images, imsave\n",
    "\n",
    "import cv2\n",
    "from cv2 import dnn_superres\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0adb5-57b8-4b80-ba07-b09212db75fa",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09db3a01-4beb-4436-8d6d-1a35c2db815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(modelType = None, res = None, folder = None):\n",
    "    r\"\"\"starts the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config = load_config(modelType, res, folder)\n",
    "    \n",
    "    # cuda visble devices\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(e) for e in config.GPU)\n",
    "\n",
    "\n",
    "    # init device\n",
    "    if torch.cuda.is_available():\n",
    "        config.DEVICE = torch.device(\"cuda\")\n",
    "        torch.backends.cudnn.benchmark = True   # cudnn auto-tuner\n",
    "    else:\n",
    "        config.DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    # set cv2 running threads to 1 (prevents deadlocks with pytorch dataloader)\n",
    "    cv2.setNumThreads(0)\n",
    "\n",
    "    # initialize random seed\n",
    "    torch.manual_seed(config.SEED)\n",
    "    torch.cuda.manual_seed_all(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    random.seed(config.SEED)\n",
    "\n",
    "    # build the model and initialize\n",
    "    model = EdgeConnect(config)\n",
    "\n",
    "    model.load()\n",
    "\n",
    "    # model test\n",
    "    print('begin redaction...\\n')\n",
    "    model.test(folder)\n",
    "\n",
    "    ######################## remove input / output folders ###################################\n",
    "    inputPath = 'examples/input/' + folder\n",
    "    inputPath = os.path.join(os.getcwd(), inputPath)\n",
    "    print(\"Removing \" + inputPath + \"...\")\n",
    "    shutil.rmtree(inputPath)\n",
    "    \n",
    "    \n",
    "    outputPath = 'examples/output/' + folder\n",
    "    outputPath = os.path.join(os.getcwd(), outputPath)\n",
    "    print(\"Removing \" + outputPath + \"...\")\n",
    "    shutil.rmtree(outputPath)\n",
    "    ######################## remove input / output folders ###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d8f59-f45f-405a-a66e-e5f82bf8e482",
   "metadata": {},
   "source": [
    "## Load Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048a69ad-c808-41c8-9909-29ba3bd2e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(modelType=None, res = None, folder = None):\n",
    "    r\"\"\"loads model config\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # cpu mode\n",
    "    if res == \"cpu\":\n",
    "        config_dict = {\n",
    "            \"cpu\": 'yes', \n",
    "            \"edge\": None, \n",
    "            \"input\": './examples/input/' + folder, \n",
    "            \"model\": modelType, \n",
    "            \"output\": './examples/output/' + folder, \n",
    "            \"path\": './checkpoints', \n",
    "            \"remove\": [15]\n",
    "                   }\n",
    "        args = Namespace(**config_dict)\n",
    "        print(args)\n",
    "\n",
    "    # gpu mode\n",
    "    elif res == \"gpu\":\n",
    "        config_dict = {\n",
    "            \"cpu\": None, \n",
    "            \"edge\": None, \n",
    "            \"input\": './examples/input/' + folder, \n",
    "            \"model\": modelType, \n",
    "            \"output\": './examples/output/' + folder, \n",
    "            \"path\": './checkpoints', \n",
    "            \"remove\": [15],\n",
    "                    }\n",
    "        args = Namespace(**config_dict)\n",
    "        \n",
    "        print(\"Machine Configuration\")\n",
    "        print(args, \"\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(\"please input either cpu or gpu as your mode\")   \n",
    "\n",
    "    ######################## create frames ########################\n",
    "    vidcap = cv2.VideoCapture('./examples/source/' + folder + '.mp4')\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    inputPath = 'examples/input/' + folder\n",
    "    inputPath = os.path.join(os.getcwd(), inputPath)\n",
    "    try:  \n",
    "        os.mkdir(inputPath)  \n",
    "    except OSError as error:  \n",
    "        print(\"path already exists\") \n",
    "    while success:\n",
    "      cv2.imwrite('./examples/input/' + folder + '/' + \"frame%07d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "      success,image = vidcap.read()\n",
    "      count += 1\n",
    "    ######################## create frames ########################\n",
    "    \n",
    "    #if path for checkpoint not given\n",
    "    if args.path is None:\n",
    "        args.path='./checkpoints'\n",
    "    config_path = os.path.join(args.path, 'config.yml')\n",
    "    \n",
    "       # create checkpoints path if does't exist\n",
    "    if not os.path.exists(args.path):\n",
    "        os.makedirs(args.path)\n",
    "\n",
    "    # copy config template if does't exist\n",
    "    if not os.path.exists(config_path):\n",
    "        copyfile('./config.yml.example', config_path)\n",
    "\n",
    "    # load config file\n",
    "    config = Config(config_path)\n",
    "   \n",
    "    # eval mode\n",
    "    config.MODE = 3 # 1 train, 2 test, 3 eval\n",
    "    config.MODEL = args.model if args.model is not None else 3\n",
    "    config.OBJECTS = args.remove if args.remove is not None else [15]\n",
    "    config.SEG_DEVICE = 'cpu' if args.cpu is not None else 'cuda'\n",
    "    config.INPUT_SIZE = 256\n",
    "\n",
    "    # inpout PATH\n",
    "    if args.input is not None:\n",
    "        config.TEST_FLIST = args.input\n",
    "    \n",
    "    if args.edge is not None:\n",
    "        config.TEST_EDGE_FLIST = args.edge\n",
    "\n",
    "    # output PATH\n",
    "    if args.output is not None:\n",
    "        config.RESULTS = args.output\n",
    "    else: \n",
    "        if not os.path.exists('./results_images'):\n",
    "            os.makedirs('./results_images')\n",
    "        config.RESULTS = './results_images'\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075cb30-c51f-4b9b-a23e-3439f0ddea3a",
   "metadata": {},
   "source": [
    "## Split Video into a sequence of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a5cdbca-f6fa-48b5-873f-7bf4cbf1e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, config, flist, edge_flist, augment=True, training=True):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.augment = augment\n",
    "        self.training = training\n",
    "        self.data = self.load_flist(flist)\n",
    "        self.edge_data = self.load_flist(edge_flist)\n",
    "\n",
    "        self.input_size = config.INPUT_SIZE\n",
    "        self.sigma = config.SIGMA\n",
    "        self.edge = config.EDGE\n",
    "        self.mask = config.MASK\n",
    "        self.nms = config.NMS\n",
    "        self.device = config.SEG_DEVICE\n",
    "        self.objects = config.OBJECTS\n",
    "        self.segment_net = config.SEG_NETWORK\n",
    "        # in test mode, there's a one-to-one relationship between mask and image\n",
    "        # masks are loaded non random\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            item = self.load_item(index)\n",
    "        except:\n",
    "            print('loading error: ' + self.data[index])\n",
    "            item = self.load_item(0)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def load_name(self, index):\n",
    "        name = self.data[index]\n",
    "        return os.path.basename(name)\n",
    "        \n",
    "    def load_size(self, index):\n",
    "        img = Image.open(self.data[index])\n",
    "        width,height=img.size\n",
    "        return width,height\n",
    "\n",
    "\n",
    "    def load_item(self, index):\n",
    "\n",
    "        size = self.input_size\n",
    "\n",
    "        # load image\n",
    "        img = Image.open(self.data[index])\n",
    "        \n",
    "        \n",
    "        # gray to rgb\n",
    "        if img.mode !='RGB':\n",
    "            img = gray2rgb(np.array(img))\n",
    "            img=Image.fromarray(img)\n",
    "\n",
    "        # resize/crop if needed\n",
    "        img,mask=segmentor(self.segment_net,img,self.device,self.objects)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        # # print(\"show image from dataset before resize\")\n",
    "        # plt.imshow(img); plt.show()\n",
    "\n",
    "\n",
    "        ######################### Determine aspect ratio ######################  \n",
    "        # find aspect ratio\n",
    "        width_og, height_og = img.size\n",
    "        ratio = (width_og/height_og)\n",
    "\n",
    "        if (ratio < 1):    \n",
    "            size_W = int(size)\n",
    "            size_H = int(452)      \n",
    "        elif (ratio > 1):\n",
    "            size_W = int(452)\n",
    "            size_H = int(size)\n",
    "        else:\n",
    "            size_W = (size)\n",
    "            size_H = (size)\n",
    "        ######################### Determine aspect ratio ######################  \n",
    "        \n",
    "        # resize to square image\n",
    "        img = np.array(img.resize((size_W, size_H), Image.LANCZOS))\n",
    "\n",
    "        # print(\"show image from dataset after resize\")\n",
    "        # plt.imshow(img); plt.show()\n",
    "\n",
    "        # create grayscale image\n",
    "        img_gray = rgb2gray(np.array(img))\n",
    "\n",
    "        # load mask\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        # resize to square image\n",
    "        mask = np.array(mask.resize((size_W, size_H), Image.LANCZOS))\n",
    "\n",
    "        # # print(\"show mask from dataset\")\n",
    "        # plt.imshow(mask); plt.show()\n",
    "\n",
    "        idx=(mask>0)\n",
    "        mask[idx]=255\n",
    "        #kernel = np.ones((5, 5), np.uint8)\n",
    "        #opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        #closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "        mask=np.apply_along_axis(fill_gaps, 1, mask) #horizontal padding\n",
    "        mask=np.apply_along_axis(fill_gaps, 0, mask) #vertical padding\n",
    "        \n",
    "        # load edge\n",
    "        edge = self.load_edge(img_gray, index, mask)\n",
    "\n",
    "        # augment data\n",
    "        if self.augment and np.random.binomial(1, 0.5) > 0:\n",
    "            img = img[:, ::-1, ...]\n",
    "            img_gray = img_gray[:, ::-1, ...]\n",
    "            edge = edge[:, ::-1, ...]\n",
    "            mask = mask[:, ::-1, ...]\n",
    "\n",
    "        return self.to_tensor(img), self.to_tensor(img_gray), self.to_tensor(edge), self.to_tensor(mask)\n",
    "\n",
    "    def load_edge(self, img, index, mask):\n",
    "        sigma = self.sigma\n",
    "\n",
    "        # in test mode images are masked (with masked regions),\n",
    "        # using 'mask' parameter prevents canny to detect edges for the masked regions\n",
    "        mask = None if self.training else (1 - mask / 255).astype(np.bool)\n",
    "        \n",
    "        # canny\n",
    "        if self.edge == 1:\n",
    "            # no edge\n",
    "            if sigma == -1:\n",
    "                return np.zeros(img.shape).astype(np.float)\n",
    "\n",
    "            # random sigma\n",
    "            if sigma == 0:\n",
    "                sigma = random.randint(1, 4)\n",
    "\n",
    "            return canny(img, sigma=sigma, mask=mask).astype(np.float)\n",
    "\n",
    "        # external\n",
    "        else:\n",
    "            imgh, imgw = img.shape[0:2]\n",
    "            edge = imread(self.edge_data[index])\n",
    "            edge = self.resized(edge, imgh, imgw)\n",
    "\n",
    "            # non-max suppression\n",
    "            if self.nms == 1:\n",
    "                edge = edge * canny(img, sigma=sigma, mask=mask)\n",
    "\n",
    "            return edge\n",
    "\n",
    "    \n",
    "    def to_tensor(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img_t = F.to_tensor(img).float()\n",
    "        return img_t\n",
    "\n",
    "\n",
    "    def load_flist(self, flist):\n",
    "        if isinstance(flist, list):\n",
    "            return flist\n",
    "\n",
    "        # flist: image file path, image directory path, text file flist path\n",
    "        if isinstance(flist, str):\n",
    "            if os.path.isdir(flist):\n",
    "                flist = list(glob.glob(flist + '/*.jpg')) + list(glob.glob(flist + '/*.png'))\n",
    "                flist.sort()\n",
    "                return flist\n",
    "\n",
    "            if os.path.isfile(flist):\n",
    "                try:\n",
    "                    return np.genfromtxt(flist, dtype=np.str, encoding='utf-8')\n",
    "                except:\n",
    "                    return [flist]\n",
    "\n",
    "        return []\n",
    "\n",
    "    def create_iterator(self, batch_size):\n",
    "        while True:\n",
    "            sample_loader = DataLoader(\n",
    "                dataset=self,\n",
    "                batch_size=batch_size,\n",
    "                drop_last=True\n",
    "            )\n",
    "\n",
    "            for item in sample_loader:\n",
    "                yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5fb4c7-765c-4f65-99ff-12a5d5ca1d53",
   "metadata": {},
   "source": [
    "## Apply model to each frame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc70ee2-9233-4a93-9a74-a5e402339d67",
   "metadata": {},
   "source": [
    "#### If you wish to upscale with CPU, then chnage the conditional statment: <em> if (self.config.SEG_DEVICE != \"gpu\") </em> to anything which accepts CPU input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b58bdc-7b7a-4302-9964-bfcb864d80d3",
   "metadata": {},
   "source": [
    "#### Addition note on upscaling \n",
    "\n",
    "EDSR [1]. This is the best performing model. However, it is also the biggest model and therefor has the biggest file size and slowest inference.\r\n",
    "\n",
    "ESPCN [2]. This is a small model with fast and good inference. It can do real-time video upscaling (depending on image size).\n",
    "\r\n",
    "FSRCNN [3]. This is also small model with fast and accurate inferenc\n",
    "\n",
    ".\r\n",
    "LapSRN [4]. This is a medium sized mo.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7858bf9-5018-4b80-8d3c-6e011fb7a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConnect():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        if config.MODEL == 1:\n",
    "            model_name = 'edge'\n",
    "        elif config.MODEL == 2:\n",
    "            model_name = 'inpaint'\n",
    "        elif config.MODEL == 3:\n",
    "            model_name = 'edge_inpaint'\n",
    "        elif config.MODEL == 4:\n",
    "            model_name = 'joint'\n",
    "\n",
    "        self.debug = False\n",
    "        self.model_name = model_name\n",
    "        self.edge_model = EdgeModel(config).to(config.DEVICE)\n",
    "        self.inpaint_model = InpaintingModel(config).to(config.DEVICE)\n",
    "\n",
    "        # test mode\n",
    "        self.test_dataset = Dataset(config, config.TEST_FLIST, config.TEST_EDGE_FLIST, augment=False, training=False)\n",
    "      \n",
    "\n",
    "        self.samples_path = os.path.join(config.PATH, 'samples')\n",
    "        \n",
    "        self.results_path = os.path.join(config.PATH, 'results')\n",
    "\n",
    "        if config.RESULTS is not None:\n",
    "            self.results_path = os.path.join(config.RESULTS)\n",
    "\n",
    "        if config.DEBUG is not None and config.DEBUG != 0:\n",
    "            self.debug = True\n",
    "\n",
    "        self.log_file = os.path.join(config.PATH, 'log_' + model_name + '.dat')\n",
    "\n",
    "    def load(self):\n",
    "        if self.config.MODEL == 1:\n",
    "            self.edge_model.load()\n",
    "\n",
    "        elif self.config.MODEL == 2:\n",
    "            self.inpaint_model.load()\n",
    "\n",
    "        else:\n",
    "            self.edge_model.load()\n",
    "            self.inpaint_model.load()\n",
    "\n",
    "    def save(self):\n",
    "        if self.config.MODEL == 1:\n",
    "            self.edge_model.save()\n",
    "\n",
    "        elif self.config.MODEL == 2 or self.config.MODEL == 3:\n",
    "            self.inpaint_model.save()\n",
    "\n",
    "        else:\n",
    "            self.edge_model.save()\n",
    "            self.inpaint_model.save()\n",
    "\n",
    "\n",
    "    def test(self, folder):\n",
    "        self.edge_model.eval()\n",
    "        self.inpaint_model.eval()\n",
    "\n",
    "        model = self.config.MODEL\n",
    "        create_dir(self.results_path)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=self.test_dataset,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "\n",
    "         ######################### upscale for GPU use ######################   \n",
    "        if (self.config.SEG_DEVICE != \"cpu\"):\n",
    "\n",
    "            # Create an SR object\n",
    "            sr = dnn_superres.DnnSuperResImpl_create()\n",
    "         ######################### upscale for GPU use ######################  \n",
    "        \n",
    "        index = 0\n",
    "        for items in test_loader:        \n",
    "            name = self.test_dataset.load_name(index)\n",
    "            \n",
    "            images, images_gray, edges, masks = self.cuda(*items)\n",
    "            index += 1\n",
    "\n",
    "            # edge model\n",
    "            if model == 1:\n",
    "                outputs = self.edge_model(images_gray, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (edges * (1 - masks))\n",
    "\n",
    "            # inpaint model\n",
    "            elif model == 2:\n",
    "                outputs = self.inpaint_model(images, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "            # inpaint with edge model / joint model\n",
    "            else:\n",
    "                edges = self.edge_model(images_gray, edges, masks).detach()\n",
    "                outputs = self.inpaint_model(images, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "            output = self.postprocess(outputs_merged)[0]\n",
    "\n",
    "            # print(\"load original image and get size\")\n",
    "            img = Image.open(self.test_dataset.__dict__['data'][0])\n",
    "            # print(self.test_dataset.__dict__['data'][0])\n",
    "            width_og, height_og = img.size\n",
    "\n",
    "            path = os.path.join(self.results_path, name)\n",
    "            print(index, name)\n",
    "            imsave(output, path)\n",
    "            \n",
    "            ######################### upscale for GPU use ######################  \n",
    "            if (self.config.SEG_DEVICE != \"cpu\"):\n",
    "                # Read image\n",
    "                image_sr = cv2.imread(path)\n",
    "                # Read the desired model\n",
    "                # model_sr = \"EDSR_x3.pb\"\n",
    "                model_sr = \"ESPCN_x4.pb\"\n",
    "                sr.readModel(model_sr)\n",
    "                # Set the desired model and scale to get correct pre- and post-processing\n",
    "                sr.setModel(\"espcn\", 4)\n",
    "                # Upscale the image\n",
    "                result_sr = sr.upsample(image_sr)\n",
    "                # Save the image\n",
    "                cv2.imwrite(path, result_sr)\n",
    "            ######################### upscale for GPU use ######################  \n",
    "\n",
    "            if self.debug:\n",
    "                edges = self.postprocess(1 - edges)[0]\n",
    "                masked = self.postprocess(images * (1 - masks) + masks)[0]\n",
    "                fname, fext = name.split('.')\n",
    "\n",
    "                imsave(edges, os.path.join(self.results_path, fname + '_edge.' + fext))\n",
    "                imsave(masked, os.path.join(self.results_path, fname + '_masked.' + fext))\n",
    "\n",
    "        ########################## create output video ##############################\n",
    "        outDirectory = 'examples/output/'\n",
    "        \n",
    "        outPath = outDirectory + folder + '/*.jpg'\n",
    "        outPath = os.path.join(os.getcwd(), outPath)\n",
    "        \n",
    "        img = Image.open('./' + outDirectory + folder + '/' + name)\n",
    "        \n",
    "        width_og, height_og = img.size\n",
    "        \n",
    "        img_array = []\n",
    "        \n",
    "        for filename in glob.glob(outPath):\n",
    "            img = cv2.imread(filename)\n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            img_array.append(img)\n",
    "        out = cv2.VideoWriter(outDirectory + ('output-' + folder + '.mp4'),cv2.VideoWriter_fourcc(*'MP4V'), 30, size)\n",
    "        for i in range(len(img_array)):\n",
    "            out.write(img_array[i])\n",
    "        out.release()\n",
    "        ########################## create output video ##############################\n",
    "        \n",
    "        print('\\nEnd redaction....')\n",
    "        return output\n",
    "\n",
    "\n",
    "    def log(self, logs):\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write('%s\\n' % ' '.join([str(item[1]) for item in logs]))\n",
    "\n",
    "    def cuda(self, *args):\n",
    "        return (item.to(self.config.DEVICE) for item in args)\n",
    "\n",
    "    def postprocess(self, img):\n",
    "        # [0, 1] => [0, 255]\n",
    "        img = img * 255.0\n",
    "        img = img.permute(0, 2, 3, 1)\n",
    "        return img.int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaab238-68bd-4d48-b1ea-2a4e1b8a310b",
   "metadata": {},
   "source": [
    "Model is ran using the main function which can be configuered to run on CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d5acf-252b-49ae-94b5-9e8ccb93ebcb",
   "metadata": {},
   "source": [
    "# Parameters: main(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2425a4-49e0-43b1-82db-edcd30905d43",
   "metadata": {},
   "source": [
    "#### <strong>x</strong> : The modol will be applied to the video - <strong> 1 </strong> : <em> edge model </em> ,  <strong> 2 </strong> : <em> inpaint model </em>,  <strong> 3 </strong> : <em> edge-inpaint model </em>,  <strong> 4 </strong> : <em> joint model </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8464a-f953-474d-b647-86372f8d14b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <strong>y</strong> : Type of machine which the redation is being performed - <strong> cpu </strong> : <em> CPU mode </em> (low freame-rate, low-resolution video), <strong> gpu </strong> : <em> GPU mode </em> (full frame-rate, full resolution via upscaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78ba3e-b76a-41e1-ac11-ef487d917b10",
   "metadata": {},
   "source": [
    "#### <strong>z</strong> : Name of source video - Source video will need to be stored in the <strong> \\examples\\source\\ </strong> folder within the repositry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48dfd5-db51-499b-bf50-6367f8ef77c3",
   "metadata": {},
   "source": [
    "Once the model has completed redaction / inpainting, then the output video will be stored in <strong> \\examples\\output\\ </strong>, sharing the same name as the source video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53594b8-4ec6-4235-94ed-cbb37fe298cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cpu='yes', edge=None, input='./examples/input/shopping1', model=3, output='./examples/output/shopping1', path='./checkpoints', remove=[15])\n",
      "Loading EdgeModel generator...\n",
      "Loading InpaintingModel generator...\n",
      "begin redaction...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gomolina\\Anaconda3\\envs\\objRemover\\lib\\site-packages\\torchvision\\transforms\\functional.py:74: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "C:\\Users\\gomolina\\Anaconda3\\envs\\objRemover\\lib\\site-packages\\torchvision\\transforms\\functional.py:70: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 frame0000000.jpg\n",
      "2 frame0000001.jpg\n",
      "3 frame0000002.jpg\n",
      "4 frame0000003.jpg\n",
      "5 frame0000004.jpg\n",
      "6 frame0000005.jpg\n",
      "7 frame0000006.jpg\n",
      "\n",
      "End redaction....\n",
      "Removing C:\\Users\\gomolina\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Automated-objects-removal-inpainter\\examples/input/shopping1...\n",
      "Removing C:\\Users\\gomolina\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Automated-objects-removal-inpainter\\examples/output/shopping1...\n"
     ]
    }
   ],
   "source": [
    "# the following function passes three arguments to the main function\n",
    "# 3 : an edge-inpaint model will be applied to the video\n",
    "# cpu : a cpu friendly version of the codebase will be ran on the video input - this will result in a slower inference time and lower-resolution output video\n",
    "# shopping2 : shopping2 is the name of the input file located in the project directory \\examples\\source\\shopping2.mp4\n",
    "\n",
    "main(3, \"cpu\", \"shopping1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19fa03-5f57-4a02-ad2c-cd2163bcf860",
   "metadata": {},
   "source": [
    "#### The below footage was passed through the model in GPU mode. I.e: main(3, \"cpu\", \"shopping2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f01514-6548-467f-b9db-c68758b68834",
   "metadata": {},
   "source": [
    "<video width=\"426\" src=\"examples\\source\\shopping2.mp4\" controls title=\"Title\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef56d2-55af-475d-81ce-97c3843d2978",
   "metadata": {},
   "source": [
    "#### The below footage is the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef127cce-42be-4698-aa07-6bcfa690afcc",
   "metadata": {},
   "source": [
    "<video width=\"426\" src=\"examples\\output\\output-shopping2.mp4 \" controls title=\"Title\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b0e51-2211-44e1-920c-fb182e4a78d2",
   "metadata": {},
   "source": [
    "### Here are both video put together to run simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ff9e2-0fa0-4db2-8096-6899e71707f5",
   "metadata": {},
   "source": [
    "<video width=\"426\" src=\"examples\\demo\\shopping2.mp4\" controls title=\"Title\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b6a3c-8f32-4379-bb36-025bee836772",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21245e3e-6f6c-45bf-adb3-9431d9ee9bdf",
   "metadata": {},
   "source": [
    "#### 1 : Source pre-trained models to include indoor door settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cc943-7e9e-40a8-a101-618344420a7d",
   "metadata": {},
   "source": [
    "\r\n",
    "The pretrained model performs at itâ€™s weakest when encountering indoor / irregular camera angled settings. Sourcing pre-trained models or training a model with labelled cctv footage is ideal so it best handles those environments when encountered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95720b1-fb8b-49cb-8e90-574eafb330d6",
   "metadata": {},
   "source": [
    "#### 2 : Ability to redact specific avatars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0a4cd-0f3e-47b1-9795-302b572f4ef8",
   "metadata": {},
   "source": [
    "\r\n",
    "Consenting parties may want to appear in the video footage once the film had pasted through the model. \r\n",
    "\r\n",
    "Solution can be configured to only redact figures who are not wearing high-vis jackets (employee of a work site) and blur everyone else (general public). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf6244-6e07-4e67-b12e-1a72d7ed79d0",
   "metadata": {},
   "source": [
    "#### 3 : Ability to redact specific avatars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e17ff-dfb0-48e7-bc50-7bdfc2b68a59",
   "metadata": {},
   "source": [
    "Feed results of the previous frame into the next frame to smooth out video \n",
    "\r\n",
    "The video output can sometimes look jittery due to applying the inpainting model to each frame individually. \r\n",
    "\r\n",
    "A way to enhance the output video would be to use the result of an impainted frame (the model has redacted the avatar and filled in the gap with environment context) and use that filled in frame as context for the next framvideo "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
